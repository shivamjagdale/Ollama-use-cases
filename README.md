# Ollama-use-cases
Ollama use cases

Asking question to the llm from the terminal :-
1) ollama help <-- Gives you a list of all the commands 
2) ollama list <-- To see all the models 
3) ollama pull model name <-- We first need to pull the model
4) ollama run model name <-- Then run the model
5) After that you can ask the questions and the model will answer. 
6) When you are done answering the questions you can exit the question asnwering phase by /bye


Accessing with Ollama Libaray in Python :-
1) import ollama
2) set up the ollama chat
3) print the response

Accessing with api endpoint :-
1) import the liabraies 
2) configure the endpoint 
3) converting dictionary into json object
4) process the response
5) handle the response
6) print the response
 
Accessing with langchain :-
1) import the liabraries
2) configure th model
3) response invoke
4) print response
